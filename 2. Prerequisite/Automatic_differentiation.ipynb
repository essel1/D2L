{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ae88de-54b9-46c1-bd35-9d3ca4ae740b",
   "metadata": {},
   "source": [
    "Calculating derivatives is the crucial step in all the optimization algorithms that we will use to train deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e7b0f-42d3-4e76-859f-46cb558f0a97",
   "metadata": {},
   "source": [
    "Working them by hand can be tedius and error-prone.\n",
    "Fortunately, all modern deep learning frameworks take this work off our plates by offering automatic differentiation/ **autograd**. \n",
    "As we pass data through each successive function, the framework builds a computational graph that tracks how each value depends on others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045b3a69-2d3c-4f6a-b2a0-82ef8397f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028bb1b-8a63-4a8c-86be-9095df57aec0",
   "metadata": {},
   "source": [
    "### A simple Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a43b750-a7a1-4e94-870f-8168f1b8d5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x.requires_grad = True\n",
    "# We can also set this attribute true in the definition as:\n",
    "# x = torch.arange(4.0, requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70e4b4-9488-4b11-b428-b2097720ad84",
   "metadata": {},
   "source": [
    "Setting the *requires_grad* attribute to true makes a place to store gradient that is respect to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e89d4b1-22aa-4125-bd54-5f00338b5c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2* torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe75881-d468-424d-a1d2-450a2205e6b9",
   "metadata": {},
   "source": [
    "Now, we can take the gradient of y with respect to x calling it's backward method and we can access the gradient with x's grad attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc8e51f-d9e9-4ea9-8f4b-15ba48d07afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec57a115-db65-498e-b58e-de649287df0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20e795-a520-4061-acef-d6a6ccc7d1c1",
   "metadata": {},
   "source": [
    "Note that PyTorch doesn't automatically reset the gradient buffer when we record a new gradient. Instead, the new gradient is added to the already-stored gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9c8826-e815-4acd-a643-545f49ae05d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  5.,  9., 13.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519b323-673a-4453-81f3-01863880115f",
   "metadata": {},
   "source": [
    "We can reset the gradient if we don't want the previous gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144fe400-52eb-4618-b4d5-1cd231e02476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a896609-c819-4cb9-b25f-0f13f1db6c70",
   "metadata": {},
   "source": [
    "#### Another Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7ff371-0733-4d87-bb2d-77ab74d628fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(5.0, requires_grad = True)\n",
    "b = torch.sum(a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457575c5-162a-40f4-b9e2-525026b4121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.], requires_grad=True)\n",
      "tensor([0., 2., 4., 6., 8.])\n"
     ]
    }
   ],
   "source": [
    "b.backward()\n",
    "print(a)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd295d-c704-4733-9cba-4cf55ffdf3ce",
   "metadata": {},
   "source": [
    "### Backward for Non-Scalar Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18619449-3a25-4864-b0cb-6ac246128665",
   "metadata": {},
   "source": [
    "The above implementation of autograd works only when the output (y) is scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3295704-01d9-4fe1-9a13-c616ed7fe974",
   "metadata": {},
   "source": [
    "When y is a vector, the most natural representation of the derivative of y with respect to a vector x is called the **Jacobian Matrix** that contains partial derivative of each component of y with respect to each component of x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c7e13-e310-4cf7-8557-137fe7fe9385",
   "metadata": {},
   "source": [
    "While Jacobian Matrix are useful in advanced machine learning techniques, more commonly we want to sum up the gradients of each component of y with respect to the full vector x, yielding a vector of the same shape as x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8680141f-d2a1-4634-8edd-54932d6a59a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27affbc4-4530-4881-b844-19ef80e77908",
   "metadata": {},
   "source": [
    "### Detaching Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b0f31-27c2-41bc-9c40-9513c140e426",
   "metadata": {},
   "source": [
    "Sometimes we want to move some calculations outside of the recorded computational graph. We might have some intermediate terms for which we don't need to compute a gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb35af-6dfe-4a99-871c-09ca2e367ae7",
   "metadata": {},
   "source": [
    "In that case, we can detach the respective computational graph from the final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de542c1-66b5-41c5-aae4-f9b138db6c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No y-gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sulav\\AppData\\Local\\Temp\\ipykernel_13000\\1931094821.py:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  if y.grad:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "if y.grad:\n",
    "    print(y.grad)\n",
    "else:\n",
    "    print(\"No y-gradients\")\n",
    "\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "176fd2f6-4d8b-48e6-8143-6c4bc08858b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No y-gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sulav\\AppData\\Local\\Temp\\ipykernel_13000\\703075083.py:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  if y.grad:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = y * x\n",
    "\n",
    "z.sum().backward()\n",
    "if y.grad:\n",
    "    print(y.grad)\n",
    "else:\n",
    "    print(\"No y-gradients\")\n",
    "\n",
    "x.grad == 3 * x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f8090-27c3-4ade-a17e-4620b506a62c",
   "metadata": {},
   "source": [
    "Detaching computation may not be fully mathematically correct. BUT,\n",
    "In deep learning, detaching is used when we donâ€™t want certain parts of the network to contribute to backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34515eda-e678-4a17-bd73-ba12cff5b362",
   "metadata": {},
   "source": [
    "### Gradients and Python Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b861e5b-8ebd-4e95-8dc3-f577b2b60c3f",
   "metadata": {},
   "source": [
    "One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of python control flow, we can still calculate the gradient of the resulting variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72557ef8-9100-4ec1-aa50-95990ed614d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2 \n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a58fbd91-94f9-4285-ba0f-5e9bdf5f821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.2669, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(size=(), requires_grad = True)\n",
    "print(a)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842584cb-244b-42fb-b960-98f0dbfad49b",
   "metadata": {},
   "source": [
    "These are the basic steps for using autograd:\n",
    "- Attach gradients to those variables with respect to which we desire derivatives\n",
    "- Record the computation of the target value\n",
    "- Execute the backpropagation function\n",
    "- Access the resulting gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
